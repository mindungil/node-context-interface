require('dotenv').config();
const express = require('express');
const path = require('path'); 
const OpenAI = require('openai');
const cors = require('cors'); 
const { encoding_for_model } = require("@dqbd/tiktoken");
const enc = encoding_for_model("gpt-3.5-turbo");

const app = express();
app.use(cors());
app.use(express.json());

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

app.use(express.static(path.join(__dirname, 'public')));

// ğŸŸ¢ ì¬ì‹œë„ í•¨ìˆ˜ - ì‘ë‹µ ë¹„ì–´ìˆì„ ë•Œë„ ì¬ì‹œë„
async function retryRequest(callback, maxRetries = 5) {
  let attempts = 0;
  while (attempts < maxRetries) {
    try {
      const response = await callback();
      const gptResult = response?.choices?.[0]?.message?.content?.trim();
      
      // ì‘ë‹µ ë¹„ì–´ ìˆëŠ” ê²½ìš° ë‹¤ì‹œ ìš”ì²­
      if (!gptResult) {
        throw new Error("GPT ì‘ë‹µì´ ë¹„ì–´ ìˆìŒ - ì¬ì‹œë„");
      }

      return response;
    } catch (error) {
      attempts++;
      console.error(`âŒ ì¬ì‹œë„ ì¤‘... (${attempts}/${maxRetries}) - ì˜¤ë¥˜: ${error.message}`);
      if (attempts >= maxRetries) throw error;
    }
  }
}

const MAX_TOKENS = 15900;
const RESERVED_FOR_RESPONSE = 1200;
const MAX_CONTEXT_TOKENS = MAX_TOKENS - RESERVED_FOR_RESPONSE;

const countTokens = (messages) => {
  let tokens = 0;
  for (const message of messages) {
    tokens += 4; // role, structure overhead
    tokens += enc.encode(message.content || "").length;
    if (message.name) tokens -= 1; // name ì‚¬ìš© ì‹œ 1í† í° ì¤„ì–´ë“¦
  }
  tokens += 2; // assistant reply priming
  return tokens;
};

let summaryText = ""; // ëˆ„ì  ìš”ì•½ ì €ì¥

app.post('/api/chat', async (req, res) => {
  const userPrompt = req.body.message;
  const previousMessages = req.body.history || [];

  // 1. ì „ì²´ ë©”ì‹œì§€ ê¸°ë°˜ ë©”ì‹œì§€ êµ¬ì„±
  let finalMessages = [
    { role: "system", content: "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•´ì¤˜" },
    ...previousMessages,
    { role: "user", content: userPrompt },
  ];

  let totalTokens = countTokens(finalMessages);
  console.log("ğŸ’¡ ì „ì²´ ë©”ì‹œì§€ í† í° ìˆ˜:", totalTokens);

  if (totalTokens > MAX_CONTEXT_TOKENS) {
    console.log("âš ï¸ í† í° ì´ˆê³¼ ê°ì§€, ìš”ì•½ ì‹œì‘...");

    // 2. ìš”ì•½ì„ ìœ„í•œ messages ì •ë¦¬
    let keptMessages = [...previousMessages];
    let removedMessages = [];

    while (
      countTokens([
        { role: "system", content: "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•´ì¤˜" },
        ...(summaryText ? [{ role: "assistant", content: `ì´ì „ ëŒ€í™” ìš”ì•½: ${summaryText}` }] : []),
        ...keptMessages,
        { role: "user", content: userPrompt },
      ]) > MAX_CONTEXT_TOKENS - 300
    ) {
      removedMessages.push(keptMessages.shift());
    }

    // 3. ìš”ì•½ ìˆ˜í–‰
    const toSummarize = [
      ...(summaryText ? [{ role: "assistant", content: summaryText }] : []),
      ...removedMessages,
    ];

    try {
      const summaryResponse = await retryRequest(() =>
        openai.chat.completions.create({
          model: "gpt-3.5-turbo",
          messages: [
            { role: "system", content: "ë‹¤ìŒ ëŒ€í™”ë¥¼ ì´ì „ ìš”ì•½ê³¼ í•¨ê»˜ ê°„ê²°í•˜ê²Œ ë‹¤ì‹œ ìš”ì•½í•´ì¤˜." },
            ...toSummarize,
          ],
          max_tokens: 300,
        })
      );
      summaryText = summaryResponse.choices[0]?.message?.content?.trim() || summaryText;
      console.log("ğŸ“Œ ëˆ„ì  ìš”ì•½ ì—…ë°ì´íŠ¸:", summaryText);
    } catch (error) {
      console.error("âŒ ìš”ì•½ ì‹¤íŒ¨:", error);
    }

    // 4. ìµœì¢… ë©”ì‹œì§€ êµ¬ì„±
    finalMessages = [
      { role: "system", content: "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•´ì¤˜" },
      ...(summaryText ? [{ role: "assistant", content: `ì´ì „ ëŒ€í™” ìš”ì•½: ${summaryText}` }] : []),
      ...keptMessages,
      { role: "user", content: userPrompt },
    ];
  }

  // 5. ìµœì¢… ë©”ì‹œì§€ í† í° ìˆ˜ í™•ì¸
  const finalTokenCount = countTokens(finalMessages);
  console.log("ğŸ“¦ ìµœì¢… ë©”ì‹œì§€ í† í° ìˆ˜:", finalTokenCount);
  console.log("ğŸ“‰ ì‘ë‹µì„ ìœ„í•œ ë‚¨ì€ í† í°:", MAX_TOKENS - finalTokenCount);

  // 6. GPT ì‘ë‹µ í˜¸ì¶œ
  try {
    const response = await retryRequest(() =>
      openai.chat.completions.create({
        model: "gpt-3.5-turbo",
        messages: finalMessages,
        max_tokens: RESERVED_FOR_RESPONSE,
      })
    );

    const gptResponse = response.choices[0].message.content;
    res.json({ message: gptResponse });
  } catch (error) {
    console.error("âŒ GPT ì‘ë‹µ ì˜¤ë¥˜:", error);
    res.status(500).send("Internal Server Error");
  }
});

app.post('/api/update-graph', async (req, res) => {  
  const { nodes, userMessage, gptMessage } = req.body;  
  const safeNodes = nodes || {};
  const existingKeywords = Object.values(safeNodes).map(node => node.keyword);

  // âœ… safeNodesì—ì„œ dialog ë“± ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±° (id, keywordë§Œ ìœ ì§€)
  const simplifiedNodes = Object.fromEntries(
    Object.entries(safeNodes).map(([id, node]) => {
      return [id, { id, keyword: node.keyword }];
    })
  );

  console.log('ğŸ“Œ ì—…ë°ì´íŠ¸ ìš”ì²­ ë°›ìŒ');
  console.log('ğŸ“‹ í˜„ì¬ ë…¸ë“œ ëª©ë¡:', existingKeywords);
  console.log('ğŸ—ºï¸ ì „ë‹¬ëœ ë…¸ë“œ ë°ì´í„°:', JSON.stringify(simplifiedNodes, null, 2));

  try {
    const response = await retryRequest(() => openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: `
          ë‹¤ìŒ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ëŒ€í™” í‚¤ì›Œë“œë„ ì¶”ì¶œí•˜ê³ ,
          ê·¸ë˜í”„ ì—…ë°ì´íŠ¸ ì •ë³´ë¥¼ ìƒì„±í•˜ì„¸ìš”.
          
          1. í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœì™€ ì¡´ì¬í•˜ëŠ” ë…¸ë“œ ëª©ë¡ì„ ì°¸ê³ í•´ì„œ, ìµœê·¼ ëŒ€í™” ë‚´ìš©ì´ ì–´ë–¤ ë…¸ë“œ(í‚¤ì›Œë“œ)ì— ë“¤ì–´ê°€ì•¼ í•˜ëŠ”ì§€ íŒë‹¨í•˜ì‹œì˜¤
          2. íŒë‹¨í•œ ê·¼ê±°ë¡œ ìµœê·¼ ëŒ€í™” ë‚´ìš©ì˜ í‚¤ì›Œë“œë¥¼ ì •í•œ ë‹¤ìŒ, ê·¸ë˜í”„ ë‚´ ì–´ë””ì— ì—°ê²°ë˜ì–´ì•¼ í• ì§€ íŒë‹¨í•˜ê³ , ê°€ì¥ ì—°ê´€ëœ ë¶€ëª¨ ë…¸ë“œë¥¼ ì°¾ì•„ ê´€ê³„ë„ ì„¤ì •í•˜ì„¸ìš”.
          3. ê´€ê³„ëŠ” í•œ ë‹¨ì–´ ë˜ëŠ” ì§§ì€ êµ¬ë¡œ í‘œí˜„í•˜ì„¸ìš”.

          ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
          \`\`\`json
          {
            "keyword": "ì¶”ì¶œëœ í‚¤ì›Œë“œ",
            "parentNodeId": "ë¶€ëª¨ ë…¸ë“œ ID",
            "relation": "ë¶€ëª¨ì™€ì˜ ê´€ê³„"
          }
          \`\`\`
        `
        },
        { role: 'user', content: `í˜„ì¬ ê·¸ë˜í”„ ìƒíƒœ: ${JSON.stringify(simplifiedNodes)}` },
        { role: 'user', content: `í˜„ì¬ ì¡´ì¬í•˜ëŠ” ë…¸ë“œ ëª©ë¡: ${JSON.stringify(existingKeywords)}` },
        { role: 'user', content: `ìµœê·¼ ëŒ€í™” ë‚´ìš©: ${JSON.stringify({ userMessage, gptMessage })}` }
      ],
      max_tokens: 1200,
      response_format: { type: "json_object" } 
    }));

    console.log("\nğŸ“ [GPT ì‘ë‹µ ì›ë³¸ - /api/update-graph]:", response.choices[0].message.content);
     
    let gptResult = response.choices[0]?.message?.content?.trim();
    
    if (!gptResult) {
      console.error("âŒ GPT ì‘ë‹µì´ ë¹„ì–´ ìˆìŒ! ì¬ì‹œë„ ì¤‘...");
      throw new Error("Empty GPT response");
    }

    let parsedResult;
    try {
      parsedResult = JSON.parse(gptResult);
    } catch (parseError) {
      console.error("âŒ JSON íŒŒì‹± ì˜¤ë¥˜:", parseError);
      throw new Error("GPT ì‘ë‹µì„ JSONìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ");
    }

    let keyword = parsedResult.keyword?.trim() || "???";
    let parentNodeId = parsedResult.parentNodeId?.trim() || "root";
    let relation = parsedResult.relation?.trim() || "ê´€ë ¨";

    if (!Object.keys(safeNodes).includes(parentNodeId)) {
      parentNodeId = Object.keys(safeNodes).find(key => keyword.includes(safeNodes[key].keyword)) || "root";
    }

    console.log(`âœ… í‚¤ì›Œë“œ: ${keyword}, ì„ íƒëœ ë¶€ëª¨ ë…¸ë“œ: ${parentNodeId}, ê´€ê³„: ${relation}`);
    
    res.json({ keyword, parentNodeId, relation });

  } catch (error) {
    console.error("âŒ Error in Graph Update:", error);
    res.status(500).json({ error: "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ" });
  }
});

app.listen(8080, function () {
  console.log('ğŸš€ Server is listening on port 8080');
});
